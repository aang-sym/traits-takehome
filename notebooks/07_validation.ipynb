{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6092ffe8",
   "metadata": {},
   "source": [
    "## Validation Approach\n",
    "\n",
    "This notebook runs basic tests on:\n",
    "\n",
    "1. **Source data integrity** - Are all expected files present and parseable?\n",
    "2. **Preprocessing correctness** - Do aggregated CSVs match source data?\n",
    "3. **Metric calculation accuracy** - Do sprint/run/press aggregations produce sensible ranges?\n",
    "4. **Cross-metric consistency** - Do related metrics correlate as expected?\n",
    "\n",
    "For production deployment, these would be automated as pytest tests with proper assertions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cae3a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Source Data Availability ===\n",
      "\n",
      "Expected matches: 10\n",
      "✓ All source files present\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "data_dir = Path('../data')\n",
    "processed_dir = Path('../output')\n",
    "matches_dir = data_dir / 'opendata/data/matches'\n",
    "\n",
    "print(\"=== Test 1: Source Data Availability ===\\n\")\n",
    "\n",
    "# Check all 10 matches have required files\n",
    "with open(data_dir / 'opendata/data/matches.json', 'r') as f:\n",
    "    matches_meta = json.load(f)\n",
    "\n",
    "match_ids = [str(m['id']) for m in matches_meta]\n",
    "print(f\"Expected matches: {len(match_ids)}\")\n",
    "\n",
    "missing_files = []\n",
    "for match_id in match_ids:\n",
    "    match_dir = matches_dir / match_id\n",
    "    required = [\n",
    "        f\"{match_id}_match.json\",\n",
    "        f\"{match_id}_tracking_extrapolated.jsonl\",\n",
    "        f\"{match_id}_dynamic_events.csv\",\n",
    "        f\"{match_id}_phases_of_play.csv\"\n",
    "    ]\n",
    "    \n",
    "    for filename in required:\n",
    "        if not (match_dir / filename).exists():\n",
    "            missing_files.append(f\"{match_id}/{filename}\")\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"✗ Missing {len(missing_files)} files:\")\n",
    "    for f in missing_files[:5]:\n",
    "        print(f\"  - {f}\")\n",
    "else:\n",
    "    print(\"✓ All source files present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd64df85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test 2: Preprocessed Data Consistency ===\n",
      "\n",
      "Events loaded: 47,853\n",
      "Phases loaded: 4,581\n",
      "Player-match records: 360\n",
      "\n",
      "Match coverage:\n",
      "  Events: 10/10\n",
      "  Phases: 10/10\n",
      "  Metadata: 10/10\n",
      "✓ All preprocessed files cover 10 matches\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Test 2: Preprocessed Data Consistency ===\\n\")\n",
    "\n",
    "# Load preprocessed files\n",
    "all_events = pd.read_csv(processed_dir / 'all_events.csv', low_memory=False)\n",
    "all_phases = pd.read_csv(processed_dir / 'all_phases.csv')\n",
    "player_metadata = pd.read_csv(processed_dir / 'player_metadata.csv')\n",
    "\n",
    "# Check row counts match expectations\n",
    "print(f\"Events loaded: {len(all_events):,}\")\n",
    "print(f\"Phases loaded: {len(all_phases):,}\")\n",
    "print(f\"Player-match records: {len(player_metadata):,}\")\n",
    "\n",
    "# Check match coverage\n",
    "events_matches = all_events['match_id'].nunique()\n",
    "phases_matches = all_phases['match_id'].nunique()\n",
    "meta_matches = player_metadata['match_id'].nunique()\n",
    "\n",
    "print(f\"\\nMatch coverage:\")\n",
    "print(f\"  Events: {events_matches}/10\")\n",
    "print(f\"  Phases: {phases_matches}/10\")\n",
    "print(f\"  Metadata: {meta_matches}/10\")\n",
    "\n",
    "if events_matches == phases_matches == meta_matches == 10:\n",
    "    print(\"✓ All preprocessed files cover 10 matches\")\n",
    "else:\n",
    "    print(\"✗ Inconsistent match coverage across files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f248bfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test 3: Sprint Detection Sanity Checks ===\n",
      "\n",
      "Sprint volume distribution:\n",
      "count    244.00\n",
      "mean       7.55\n",
      "std        4.56\n",
      "min        0.88\n",
      "25%        4.31\n",
      "50%        6.59\n",
      "75%        9.82\n",
      "max       29.37\n",
      "Name: sprints_per_90, dtype: float64\n",
      "✓ Sprint volumes in realistic range\n",
      "\n",
      "Sprint speeds:\n",
      "  Avg: 25.6 - 28.4 km/h\n",
      "  Max: 26.1 - 31.1 km/h\n",
      "✓ Sprint speeds physically plausible\n",
      "\n",
      "Context quality:\n",
      "  High-value phase: 63.0%\n",
      "  Attacking sprints: 39.1%\n",
      "✓ High-value sprint clustering matches expectations\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Test 3: Sprint Detection Sanity Checks ===\\n\")\n",
    "\n",
    "player_sprints = pd.read_csv(processed_dir / 'player_sprints.csv')\n",
    "\n",
    "# Check sprint volumes are realistic\n",
    "sprints_per_90 = player_sprints['sprints_per_90']\n",
    "\n",
    "print(\"Sprint volume distribution:\")\n",
    "print(sprints_per_90.describe().round(2))\n",
    "\n",
    "# Expected ranges based on professional benchmarks\n",
    "if sprints_per_90.median() < 3 or sprints_per_90.median() > 20:\n",
    "    print(\"⚠ Median sprints per 90 outside expected range (3-20)\")\n",
    "else:\n",
    "    print(\"✓ Sprint volumes in realistic range\")\n",
    "\n",
    "# Check speeds are physically plausible\n",
    "avg_speeds = player_sprints['avg_sprint_speed_kmh']\n",
    "max_speeds = player_sprints['max_sprint_speed_kmh']\n",
    "\n",
    "print(f\"\\nSprint speeds:\")\n",
    "print(f\"  Avg: {avg_speeds.min():.1f} - {avg_speeds.max():.1f} km/h\")\n",
    "print(f\"  Max: {max_speeds.min():.1f} - {max_speeds.max():.1f} km/h\")\n",
    "\n",
    "if avg_speeds.max() > 30 or max_speeds.max() > 36:\n",
    "    print(\"⚠ Some sprint speeds exceed human capability\")\n",
    "else:\n",
    "    print(\"✓ Sprint speeds physically plausible\")\n",
    "\n",
    "# Check context quality percentages sum correctly\n",
    "high_value_pct = player_sprints['high_value_sprint_pct'].mean()\n",
    "attacking_pct = player_sprints['attacking_sprint_pct'].mean()\n",
    "\n",
    "print(f\"\\nContext quality:\")\n",
    "print(f\"  High-value phase: {high_value_pct:.1%}\")\n",
    "print(f\"  Attacking sprints: {attacking_pct:.1%}\")\n",
    "\n",
    "if 0.5 < high_value_pct < 0.8:\n",
    "    print(\"✓ High-value sprint clustering matches expectations\")\n",
    "else:\n",
    "    print(\"⚠ High-value sprint % outside expected range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62d6b4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test 4: Off-Ball Run Aggregation Checks ===\n",
      "\n",
      "Run threat distribution:\n",
      "count    246.0000\n",
      "mean       0.0206\n",
      "std        0.0195\n",
      "min        0.0001\n",
      "25%        0.0048\n",
      "50%        0.0153\n",
      "75%        0.0322\n",
      "max        0.1028\n",
      "Name: avg_xthreat, dtype: float64\n",
      "✓ Run threat values in expected xG-like range\n",
      "\n",
      "Runs per 90 by position:\n",
      "position_group\n",
      "Wide Attacker       32.95\n",
      "Center Forward      30.44\n",
      "Midfield            24.00\n",
      "Full Back           22.45\n",
      "Central Defender    10.07\n",
      "Other                3.65\n",
      "Name: runs_per_90, dtype: float64\n",
      "✓ Position patterns match expectations\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Test 4: Off-Ball Run Aggregation Checks ===\\n\")\n",
    "\n",
    "player_runs = pd.read_csv(processed_dir / \"player_runs.csv\")\n",
    "\n",
    "# Bring in position_group from metadata for validation-only analysis\n",
    "player_meta = pd.read_csv(processed_dir / \"player_metadata.csv\")\n",
    "runs_with_pos = player_runs.merge(\n",
    "    player_meta[[\"match_id\", \"player_id\", \"position_group\"]],\n",
    "    on=[\"match_id\", \"player_id\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Check threat values are in xG-like range\n",
    "avg_xthreat = runs_with_pos[\"avg_xthreat\"]\n",
    "\n",
    "print(\"Run threat distribution:\")\n",
    "print(avg_xthreat.describe().round(4))\n",
    "\n",
    "if avg_xthreat.max() > 0.5:\n",
    "    print(\"⚠ Some runs have unrealistically high threat (>0.5)\")\n",
    "else:\n",
    "    print(\"✓ Run threat values in expected xG-like range\")\n",
    "\n",
    "# Check selection quality if available\n",
    "if \"targeted_dangerous_pct\" in runs_with_pos.columns:\n",
    "    targeted_pct = runs_with_pos[\"targeted_dangerous_pct\"].mean()\n",
    "    print(\"\\nSelection quality:\")\n",
    "    print(f\"  Targeted dangerous rate: {targeted_pct:.1%}\")\n",
    "    \n",
    "    if 0.1 < targeted_pct < 0.5:\n",
    "        print(\"✓ Selection rates realistic\")\n",
    "\n",
    "# Check position patterns make sense\n",
    "print(\"\\nRuns per 90 by position:\")\n",
    "position_runs = (\n",
    "    runs_with_pos\n",
    "    .groupby(\"position_group\")[\"runs_per_90\"]\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "print(position_runs.round(2))\n",
    "\n",
    "# Wide attackers should have more runs than defenders\n",
    "if position_runs.index[0] in [\"Wide Attacker\", \"Forward\"]:\n",
    "    print(\"✓ Position patterns match expectations\")\n",
    "else:\n",
    "    print(\"⚠ Unexpected position ordering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "018f4b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test 5: Pressing Effectiveness Checks ===\n",
      "\n",
      "Pressing success distribution:\n",
      "count    199.000\n",
      "mean       0.161\n",
      "std        0.159\n",
      "min        0.000\n",
      "25%        0.059\n",
      "50%        0.133\n",
      "75%        0.200\n",
      "max        1.000\n",
      "Name: press_success_rate, dtype: float64\n",
      "✓ Success rates are valid probabilities\n",
      "\n",
      "Outcome breakdown:\n",
      "  Overall success: 16.1%\n",
      "  Regain rate: 11.3%\n",
      "  Disruption rate: 4.8%\n",
      "✓ Outcome rates logically consistent\n",
      "\n",
      "Pressing volume: 13.4 per 90 (median)\n",
      "✓ Pressing volumes realistic\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Test 5: Pressing Effectiveness Checks ===\\n\")\n",
    "\n",
    "player_pressing = pd.read_csv(processed_dir / 'player_pressing.csv')\n",
    "\n",
    "# Check success rates are probabilistic (0-1 range)\n",
    "success_rate = player_pressing['press_success_rate']\n",
    "regain_rate = player_pressing['regain_rate']\n",
    "\n",
    "print(\"Pressing success distribution:\")\n",
    "print(success_rate.describe().round(3))\n",
    "\n",
    "if success_rate.min() < 0 or success_rate.max() > 1:\n",
    "    print(\"✗ Success rates outside 0-1 range\")\n",
    "else:\n",
    "    print(\"✓ Success rates are valid probabilities\")\n",
    "\n",
    "# Check regain rate is subset of success rate\n",
    "mean_success = success_rate.mean()\n",
    "mean_regain = regain_rate.mean()\n",
    "\n",
    "print(f\"\\nOutcome breakdown:\")\n",
    "print(f\"  Overall success: {mean_success:.1%}\")\n",
    "print(f\"  Regain rate: {mean_regain:.1%}\")\n",
    "print(f\"  Disruption rate: {player_pressing['disruption_rate'].mean():.1%}\")\n",
    "\n",
    "if mean_regain > mean_success:\n",
    "    print(\"✗ Regain rate exceeds success rate (logical error)\")\n",
    "else:\n",
    "    print(\"✓ Outcome rates logically consistent\")\n",
    "\n",
    "# Check volume is realistic\n",
    "pressing_per_90 = player_pressing['pressing_actions_per_90']\n",
    "print(f\"\\nPressing volume: {pressing_per_90.median():.1f} per 90 (median)\")\n",
    "\n",
    "if 5 < pressing_per_90.median() < 25:\n",
    "    print(\"✓ Pressing volumes realistic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0280bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test 6: Cross-Metric Consistency ===\n",
      "\n",
      "Unique players:\n",
      "  Sprints: 166\n",
      "  Runs: 171\n",
      "  Pressing: 145\n",
      "\n",
      "Total unique players: 173\n",
      "Players in all 3 metrics: 142\n",
      "Overlap rate: 82.1%\n",
      "✓ Good overlap across metrics\n",
      "\n",
      "Sprint vs Run volume correlation: 0.343\n",
      "✓ Expected positive correlation between sprint and run volume\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Test 6: Cross-Metric Consistency ===\\n\")\n",
    "\n",
    "# Load all three metrics\n",
    "sprints = pd.read_csv(processed_dir / 'player_sprints.csv')\n",
    "runs = pd.read_csv(processed_dir / 'player_runs.csv')\n",
    "pressing = pd.read_csv(processed_dir / 'player_pressing.csv')\n",
    "\n",
    "print(f\"Unique players:\")\n",
    "print(f\"  Sprints: {sprints['player_id'].nunique()}\")\n",
    "print(f\"  Runs: {runs['player_id'].nunique()}\")\n",
    "print(f\"  Pressing: {pressing['player_id'].nunique()}\")\n",
    "\n",
    "# Check overlap - most players should appear in multiple metrics\n",
    "all_player_ids = set(sprints['player_id']) | set(runs['player_id']) | set(pressing['player_id'])\n",
    "in_all_three = set(sprints['player_id']) & set(runs['player_id']) & set(pressing['player_id'])\n",
    "\n",
    "print(f\"\\nTotal unique players: {len(all_player_ids)}\")\n",
    "print(f\"Players in all 3 metrics: {len(in_all_three)}\")\n",
    "print(f\"Overlap rate: {len(in_all_three) / len(all_player_ids):.1%}\")\n",
    "\n",
    "if len(in_all_three) / len(all_player_ids) > 0.5:\n",
    "    print(\"✓ Good overlap across metrics\")\n",
    "\n",
    "# Expected correlations\n",
    "# Sprint volume and run volume should correlate (both measure movement)\n",
    "merged = sprints[['player_id', 'sprints_per_90']].merge(\n",
    "    runs[['player_id', 'runs_per_90']], \n",
    "    on='player_id'\n",
    ")\n",
    "\n",
    "if len(merged) > 10:\n",
    "    corr = merged['sprints_per_90'].corr(merged['runs_per_90'])\n",
    "    print(f\"\\nSprint vs Run volume correlation: {corr:.3f}\")\n",
    "    \n",
    "    if corr > 0.3:\n",
    "        print(\"✓ Expected positive correlation between sprint and run volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1579c5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validation Summary ===\n",
      "\n",
      "Tests completed:\n",
      "1. ✓ Source data availability\n",
      "2. ✓ Preprocessed data consistency\n",
      "3. ✓ Sprint detection accuracy\n",
      "4. ✓ Off-ball run aggregations\n",
      "5. ✓ Pressing effectiveness metrics\n",
      "6. ✓ Cross-metric consistency\n",
      "\n",
      "All basic validation checks passed.\n",
      "\n",
      "For production deployment, additional tests would include:\n",
      "- Null value patterns and expected missingness\n",
      "- Outlier detection and flagging\n",
      "- Temporal consistency (same player across matches)\n",
      "- Join key integrity (no orphaned records)\n",
      "- Performance benchmarks (processing time per match)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Validation Summary ===\\n\")\n",
    "\n",
    "print(\"Tests completed:\")\n",
    "print(\"1. ✓ Source data availability\")\n",
    "print(\"2. ✓ Preprocessed data consistency\")\n",
    "print(\"3. ✓ Sprint detection accuracy\")\n",
    "print(\"4. ✓ Off-ball run aggregations\")\n",
    "print(\"5. ✓ Pressing effectiveness metrics\")\n",
    "print(\"6. ✓ Cross-metric consistency\")\n",
    "\n",
    "print(\"\\nAll basic validation checks passed.\")\n",
    "print(\"\\nFor production deployment, additional tests would include:\")\n",
    "print(\"- Null value patterns and expected missingness\")\n",
    "print(\"- Outlier detection and flagging\")\n",
    "print(\"- Temporal consistency (same player across matches)\")\n",
    "print(\"- Join key integrity (no orphaned records)\")\n",
    "print(\"- Performance benchmarks (processing time per match)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
